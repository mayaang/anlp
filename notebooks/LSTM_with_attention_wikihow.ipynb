{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM-with-attention-wikihow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjnSyspVKVHv"
      },
      "source": [
        "# Dependencies installation and import \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHvrPj86UvMA"
      },
      "source": [
        "!pip install unidecode\n",
        "!pip install pandas\n",
        "!pip install re\n",
        "!pip install pickle\n",
        "!pip install sklearn\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEEkd5Qos20x"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import unidecode\n",
        "from string import ascii_letters, punctuation\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_ToSfhnsMgi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nepuiYxxRJ_q"
      },
      "source": [
        "# Data processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdybyLcARKAK"
      },
      "source": [
        "wikihow = pd.read_csv(r'/content/drive/MyDrive/summarization/wikihowAll.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hBWlJ_bRKAN"
      },
      "source": [
        "wikihow = wikihow.astype(str)\n",
        "wikihow=wikihow[wikihow['text'].isnull()==False]\n",
        "wikihow=wikihow[wikihow['headline'].isnull()==False]\n",
        "wikihow=wikihow[wikihow['text']!='nan']\n",
        "wikihow.drop_duplicates(subset=['text'],inplace=True)\n",
        "wikihow['word_count'] = wikihow['text'].str.count(' ') + 1\n",
        "wikihow['headline_count'] = wikihow['headline'].str.count(' ') + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTDnVfBGRKAU"
      },
      "source": [
        "path = '/content/drive/MyDrive/summarization/wikihow_all.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUxutrjKKvsL"
      },
      "source": [
        "# pd.to_pickle(wikihow, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkde0lL6KuwR"
      },
      "source": [
        "# wikihow_ = pd.read_pickle(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkBWVycBRKCK"
      },
      "source": [
        "contraction = { \n",
        "    \"ain't\": \"is not\", \n",
        "    \"aren't\": \"are not\", \n",
        "    \"can't\": \"cannot\", \n",
        "    \"'cause\": \"because\", \n",
        "    \"could've\": \"could have\", \n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\", \n",
        "    \"doesn't\": \"does not\", \n",
        "    \"don't\": \"do not\", \n",
        "    \"hadn't\": \"had not\", \n",
        "    \"hasn't\": \"has not\", \n",
        "    \"haven't\": \"have not\",\n",
        "    \"haven t\": \"have not\",\n",
        "    \"he'd\": \"he would\", \n",
        "    \"he'll\": \"he will\", \n",
        "    \"he's\": \"he is\", \n",
        "    \"how'd\": \"how did\", \n",
        "    \"how'd'y\": \"how do you\", \n",
        "    \"how'll\": \"how will\", \n",
        "    \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\", \n",
        "    \"I'd've\": \"I would have\", \n",
        "    \"I'll\": \"I will\", \n",
        "    \"I'll've\": \"I will have\", \n",
        "    \"I'm\": \"I am\", \n",
        "    \"I've\": \"I have\", \n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\", \n",
        "    \"i'll\": \"i will\", \n",
        "    \"i'll've\": \"i will have\", \n",
        "    \"i'm\": \"i am\", \n",
        "    \"i've\": \"i have\", \n",
        "    \"isn't\": \"is not\", \n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\", \n",
        "    \"it'll\": \"it will\", \n",
        "    \"it'll've\": \"it will have\", \n",
        "    \"it's\": \"it is\", \n",
        "    \"let's\": \"let us\", \n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\", \n",
        "    \"might've\": \"might have\", \n",
        "    \"mightn't\": \"might not\", \n",
        "    \"mightn't've\": \"might not have\", \n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\", \n",
        "    \"mustn't've\": \"must not have\", \n",
        "    \"needn't\": \"need not\", \n",
        "    \"needn't've\": \"need not have\", \n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\", \n",
        "    \"oughtn't've\": \"ought not have\", \n",
        "    \"shan't\": \"shall not\", \n",
        "    \"sha'n't\": \"shall not\", \n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\", \n",
        "    \"she'd've\": \"she would have\", \n",
        "    \"she'll\": \"she will\", \n",
        "    \"she'll've\": \"she will have\", \n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\", \n",
        "    \"shouldn't\": \"should not\", \n",
        "    \"shouldn't've\": \"should not have\", \n",
        "    \"so've\": \"so have\", \n",
        "    \"so's\": \"so as\",\n",
        "    \"this's\": \"this is\", \n",
        "    \"that'd\": \"that would\", \n",
        "    \"that'd've\": \"that would have\", \n",
        "    \"that's\": \"that is\", \n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\", \n",
        "    \"there's\": \"there is\", \n",
        "    \"here's\": \"here is\", \n",
        "    \"they'd\": \"they would\", \n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\", \n",
        "    \"they'll've\": \"they will have\", \n",
        "    \"they're\": \"they are\", \n",
        "    \"they've\": \"they have\", \n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\", \n",
        "    \"we'd\": \"we would\", \n",
        "    \"we'd've\": \"we would have\", \n",
        "    \"we'll\": \"we will\", \n",
        "    \"we'll've\": \"we will have\", \n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\", \n",
        "    \"weren't\": \"were not\", \n",
        "    \"what'll\": \"what will\", \n",
        "    \"what'll've\": \"what will have\", \n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\", \n",
        "    \"what've\": \"what have\", \n",
        "    \"when's\": \"when is\", \n",
        "    \"when've\": \"when have\", \n",
        "    \"where'd\": \"where did\", \n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\", \n",
        "    \"who'll\": \"who will\", \n",
        "    \"who'll've\": \"who will have\", \n",
        "    \"who's\": \"who is\", \n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\", \n",
        "    \"why've\": \"why have\", \n",
        "    \"will've\": \"will have\", \n",
        "    \"won't\": \"will not\", \n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\", \n",
        "    \"wouldn't\": \"would not\", \n",
        "    \"wouldn't've\": \"would not have\", \n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\", \n",
        "    \"y'all'd've\": \"you all would have\", \n",
        "    \"y'all're\": \"you all are\", \n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\", \n",
        "    \"you'd've\": \"you would have\", \n",
        "    \"you'll\": \"you will\", \n",
        "    \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\", \n",
        "    \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx8MbiYVRKCP"
      },
      "source": [
        "def clean(text):\n",
        "  \n",
        "    text = text.lower()\n",
        "    text = unidecode.unidecode(text)\n",
        "    text = text.replace('&', ' and ')\n",
        "    text = text.replace('@', ' at ')\n",
        "    text = ' '.join([contraction[t] if t in contraction else t for t in text.split(\" \")])\n",
        "    \n",
        "    punct = '|'.join('\\\\' + char for char in punctuation)\n",
        "    punct = r'['+punct+']'\n",
        "    text = re.sub(punct, ' ', text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\n\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\W\", \" \", text)\n",
        "    text = re.sub(r\"'s\\b\",\"\",text)\n",
        "    text = re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
        "\n",
        "    text = text.strip(\" \")\n",
        "    text = re.sub('\"','', text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = re.sub(' +',' ', text).strip() \n",
        "\n",
        "    if len(text) != 0 and text[len(text)-1].isdigit():\n",
        "        text = text[0:len(text)-1]\n",
        "    return text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUORQ4BeRKCU"
      },
      "source": [
        "wikihow[\"clean_title\"] = wikihow['title'].map(lambda text: clean(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leP_CYhuRKCd"
      },
      "source": [
        "wikihow[\"clean_text\"] = wikihow['text'].map(lambda text: clean(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x6LBZLKahaG"
      },
      "source": [
        "wikihow[\"clean_headline\"] = wikihow['headline'].map(lambda text: clean(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NM2IVVk2fIc"
      },
      "source": [
        "## Load WikiHow (test)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZhLYEXJQT9"
      },
      "source": [
        "path = '/content/drive/MyDrive/summarization/wikihow_all_clean_nn.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4BdgaYzRKDF"
      },
      "source": [
        "# pd.to_pickle(wikihow, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czm9r2CcJSAO"
      },
      "source": [
        "# wikihow = pd.read_pickle(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzb7BPk0uGRS"
      },
      "source": [
        "# Shorter articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38xYsJQMT6rf"
      },
      "source": [
        "wikihow['headline_count'] = wikihow['headline'].str.count(' ') + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsP05omCTLM7"
      },
      "source": [
        "shorty = wikihow[\"word_count\"] < MAX_INPUT_LENGTH\n",
        "shorter_articles = wikihow[shorty].copy()\n",
        "\n",
        "shortyh = shorter_articles[\"headline_count\"] < MAX_OUTPUT_LENGTH\n",
        "shorter_articles = shorter_articles[shortyh].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGoWWbOiXbxK"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8BhLDriRM_m"
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(shorter_articles['clean_text'],shorter_articles['clean_headline'],test_size=0.25,random_state=0,shuffle=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JLB_ZoMU0io"
      },
      "source": [
        "path1 = '/content/drive/MyDrive/summarization/xtrain.pkl'\n",
        "path2 = '/content/drive/MyDrive/summarization/ytrain.pkl'\n",
        "path3 = '/content/drive/MyDrive/summarization/xtest.pkl'\n",
        "path4 = '/content/drive/MyDrive/summarization/ytest.pkl'\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDet_zw3VwdX"
      },
      "source": [
        "import pickle\n",
        "def pick(path1, data):\n",
        "  with open(path1, 'wb') as f:\n",
        "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlLc_2eWCPn"
      },
      "source": [
        "def unpick(path1):\n",
        "  with open(path1, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "    return data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXO5ZupoV1ps"
      },
      "source": [
        "# pick(path1, x_train)\n",
        "# pick(path2, y_train)\n",
        "# pick(path3, x_test)\n",
        "# pick(path4, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s_7w_3nWMCj"
      },
      "source": [
        "# x_train = unpick(path1)\n",
        "# y_train = unpick(path2)\n",
        "# x_test = unpick(path3)\n",
        "# y_test = unpick(path4)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVcK0Er0NjA3"
      },
      "source": [
        "START = 0\n",
        "END = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT0hCmdcZIby"
      },
      "source": [
        "class Text:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"START\", 1: \"END\"}\n",
        "    self.n_words = 2  \n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2uvBCtacriQ"
      },
      "source": [
        "def readText(x_train, y_train):\n",
        "  textt=np.array(x_train)\n",
        "  summary=np.array(y_train)\n",
        "  pairs = [[textt[i],summary[i]] for i in range(len(textt))]\n",
        "  text = Text(textt)\n",
        "  reference_summary = Text(summary)\n",
        "  for pair in pairs:\n",
        "    text.addSentence(pair[0])\n",
        "    reference_summary.addSentence(pair[1])\n",
        "\n",
        "  return text, reference_summary, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQEMoa3jQyDz"
      },
      "source": [
        "input_text, output_text, pairs = readText(x_train, y_train )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEayA3eSu-4F"
      },
      "source": [
        "# LSTM with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAJJ8WMaNPi3"
      },
      "source": [
        "INPUT_SHORTER_LENGTH = 300\n",
        "\n",
        "MAX_INPUT_LENGTH = INPUT_SHORTER_LENGTH\n",
        "MAX_OUTPUT_LENGTH = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfJ_bgxzVmiL"
      },
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.input_size = input_size\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.LSTM = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    # TODO\n",
        "    output, hidden = self.LSTM(embedded, hidden)\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return (torch.zeros(1, 1, self.hidden_size, device=device),torch.zeros(1, 1, self.hidden_size, device=device)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqVupGeYm7Rb"
      },
      "source": [
        "class AttentionLSTM(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_INPUT_LENGTH):\n",
        "    super(AttentionLSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.dropout_p = dropout_p\n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "    self.attention = nn.Linear(self.hidden_size*2 , self.max_length)\n",
        "    self.combine_attention = nn.Linear(self.hidden_size*2 , self.hidden_size)\n",
        "    self.dropout = nn.Dropout(self.dropout_p)\n",
        "    self.LSTM = nn.LSTM(self.hidden_size, self.hidden_size)\n",
        "    self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "\n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    embedded = self.dropout(embedded)\n",
        "\n",
        "    attention_weights = F.softmax(self.attention(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n",
        "    applied_attention = torch.bmm(attention_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "    concatenation = torch.cat((embedded[0], applied_attention[0]), 1)\n",
        "    attention_result = self.combine_attention(concatenation).unsqueeze(0)\n",
        "\n",
        "    relu = F.relu(attention_result)\n",
        "    output, hidden = self.LSTM(relu, hidden)\n",
        "\n",
        "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "    return output, hidden, attention_weights\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4pHEQK31g8j"
      },
      "source": [
        "hidden_size = 300\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMUMfAJFiA1q"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F48HFvOsi0gh"
      },
      "source": [
        "encoder = EncoderLSTM(input_text.n_words, hidden_size).to(device)\n",
        "decoder = AttentionLSTM(hidden_size, output_text.n_words, dropout_p=0.1).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFkw0w3NdNpR"
      },
      "source": [
        "def train_model(encoder, decoder,input_text, output_text, n_iters, debug_every=1000, learning_rate=0.01, input_length_treshold = INPUT_SHORTER_LENGTH, path = '/content/drive/MyDrive/summarization/'):\n",
        "\n",
        "  debug_loss_total = 0  \n",
        "  plot_losses = []  \n",
        "\n",
        "  input_output_pairs = [tensorsFromPair(random.choice(pairs), input_text, output_text)\n",
        "                      for i in range(n_iters)]\n",
        "  encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "  decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "  criterion = nn.NLLLoss()\n",
        "  \n",
        "  for iter in range(1, n_iters + 1):\n",
        "      \n",
        "    training_pair = input_output_pairs[iter - 1]\n",
        "    input_tensor = training_pair[0]\n",
        "    target_tensor = training_pair[1]\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    if(input_length > input_length_treshold):\n",
        "      print('err')\n",
        "      print(input_length)\n",
        "      continue\n",
        "      \n",
        "    loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion )\n",
        "    debug_loss_total += loss\n",
        "    \n",
        "    if iter % debug_every == 0:\n",
        "      debug_loss_avg = debug_loss_total / debug_every\n",
        "      debug_loss_total = 0\n",
        "      plot_losses.append(debug_loss_avg)\n",
        "      torch.save(encoder.state_dict(), path + 'encoder_new3.dict')\n",
        "      torch.save(decoder.state_dict(), path + 'decoder_new3.dict')\n",
        "      print(' (%d %d%%) %.4f' % (iter, iter / n_iters * 100, debug_loss_avg))\n",
        "\n",
        "  showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GCmpRFEO65P"
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, learning_rate=0.01,  max_length=MAX_INPUT_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "        encoder_outputs[i] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[START]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    for i in range(target_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "        decoder_input, decoder_hidden, encoder_outputs) \n",
        "      \n",
        "        loss += criterion(decoder_output, target_tensor[i])\n",
        "        decoder_input = target_tensor[i]  # Teacher forcing\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzhoiaCFTXrP"
      },
      "source": [
        "def tensorsFromPair(pair, input_text, output_text):\n",
        "    input_indexes = [input_text.word2index[word] for word in pair[0].split(' ')]\n",
        "    input_indexes.append(END)\n",
        "    input_tensor = torch.tensor(input_indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "    target_indexes = [output_text.word2index[word] for word in pair[1].split(' ')]\n",
        "    target_indexes.append(END)\n",
        "    target_tensor = torch.tensor(target_indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs3wu28ydLtt"
      },
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(base=0.2))\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJiQuW4311mV"
      },
      "source": [
        "train_model(encoder, decoder, input_text, output_text, 122000, debug_every=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKtIfIM8gG1N"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO94wpqTe7PZ"
      },
      "source": [
        "def evaluate(encoder, decoder, pairs, input_test, output_test, debug_every=1000, path = '/content/drive/MyDrive/summarization/'):\n",
        "  text=[]\n",
        "  headline=[]\n",
        "  predicted_headline=[] \n",
        "  path1 =  path + 'text_lstm.pkl'\n",
        "  path2 =  path + 'headline_lstm.pkl'\n",
        "  path3 =  path + 'summary_lstm.pkl'\n",
        "\n",
        "  n = len(pairs)\n",
        "  print('Producing summaries: ')\n",
        "  count = 0\n",
        "  for i in range(0, n):\n",
        "    pair = pairs[i]\n",
        "        \n",
        "    if len(pair[0].split()) >= MAX_INPUT_LENGTH :\n",
        "      print(i)\n",
        "      print('err')\n",
        "      continue\n",
        "    else:\n",
        "      if i%debug_every == 0 :\n",
        "        print(i*100/n,\"% complete\")\n",
        "        print(i)\n",
        "        pick(path1, text)\n",
        "        pick(path2, headline)\n",
        "        pick(path3, predicted_headline)\n",
        "\n",
        "      text.append(pair[0])\n",
        "      headline.append(pair[1])\n",
        "      predicted_words, attentions = produceSummary(encoder, decoder, input_test, output_test, pair[0])\n",
        "      predicted_sentence = ' '.join(predicted_words)\n",
        "      predicted_headline.append(predicted_sentence)\n",
        "\n",
        "      count += 1\n",
        "\n",
        "  return (text,headline,predicted_headline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7i2tbx2dTNv"
      },
      "source": [
        "def produceSummary(encoder, decoder, input_text, summary_text, sentence, max_length=MAX_INPUT_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        indexes = [input_text.word2index[word] for word in sentence.split(' ')]\n",
        "        indexes.append(END)\n",
        "        input_tensor = torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for e in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[e],encoder_hidden)\n",
        "            encoder_outputs[e] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[START]], device=device)  \n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for d in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[d] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == END:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(summary_text.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:d + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjqqFj7tAI4e"
      },
      "source": [
        "input_test, output_test, pairs_test = readText(x_test, y_test )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uH0qi_PS2fv"
      },
      "source": [
        "path =  '/content/drive/MyDrive/summarization/summaries.pkl'\n",
        "summaries=pd.DataFrame()\n",
        "\n",
        "summaries['clean_text'] = \"\"\n",
        "summaries['clean_headline'] = \"\"\n",
        "summaries['summary'] = \"\"\n",
        "\n",
        "text, headline, predicted_headline = evaluate(encoder, decoder, pairs_test, input_test, output_test)\n",
        "\n",
        "\n",
        "\n",
        "summaries['clean_text']=text\n",
        "summaries['clean_headline']=headline\n",
        "summaries['summary']=predicted_headline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJYQZ-ir8yIo"
      },
      "source": [
        "path =  '/content/drive/MyDrive/summarization/summaries.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzFC9s67lBFh"
      },
      "source": [
        "pick(path, summaries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PrIOdh8XyBZ"
      },
      "source": [
        "## Installation von nlg-eval, functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtAwyvV_XxSw"
      },
      "source": [
        "!pip3 install git+https://github.com/Maluuba/nlg-eval.git@master\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyEgBo64X22e"
      },
      "source": [
        "!nlg-eval --setup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcvs5ZEAX6EW"
      },
      "source": [
        "import io\n",
        "\n",
        "def write_list_to(filename, da_list):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in da_list:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "def calculate_bleurt(filename):\n",
        "  with io.open(filename,'r') as f:\n",
        "    bl = f.readlines()\n",
        "\n",
        "  b = [float(i) for i in bl]\n",
        "  score = sum(b)/len(b)\n",
        "  print('score for ' + filename)\n",
        "  print(score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj_g9nSBYBQ5"
      },
      "source": [
        "to_eval_hyp = summaries['summary']\n",
        "to_eval_ref = summaries['clean_headline']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp5y_HNxYIUl"
      },
      "source": [
        "h_filename = '/content/drive/MyDrive/summarization/evaluation/lstm_hypothesis.txt'\n",
        "r_filename = '/content/drive/MyDrive/summarization/evaluation/lstm_reference.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3QVlcpYYOHH"
      },
      "source": [
        "write_list_to(h_filename, to_eval_hyp)\n",
        "write_list_to(r_filename, to_eval_ref)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3VetXv3YP6M"
      },
      "source": [
        "  !nlg-eval --hypothesis=/content/drive/MyDrive/summarization/evaluation/lstm_hypothesis.txt --references=/content/drive/MyDrive/summarization/evaluation/lstm_reference.txt --no-skipthoughts --no-glove "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}